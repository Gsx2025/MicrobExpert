{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb565b3b-c3c9-4349-9d34-58321d10647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import ahocorasick\n",
    "import re\n",
    "\n",
    "# 加载疾病词典\n",
    "def load_disease_dict_from_csv(disease_dict_path):\n",
    "    df = pd.read_csv(disease_dict_path)\n",
    "    diseases = df['disease_name'].dropna().unique().tolist()\n",
    "    diseases = [f' {disease.strip().lower()} ' for disease in diseases]\n",
    "    return diseases\n",
    "\n",
    "# 从文本文件加载微生物字典\n",
    "def load_microbe_dict(file_path):\n",
    "    microbes = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            microbes.append(f' {line.strip().lower()} ')  # 加入空格防止匹配子串\n",
    "    return microbes\n",
    "\n",
    "# 构建Aho-Corasick自动机\n",
    "def build_automaton(entity_list):\n",
    "    automaton = ahocorasick.Automaton()\n",
    "    sorted_entity_list = sorted(entity_list, key=len, reverse=True)  # 长实体优先\n",
    "    for idx, entity in enumerate(sorted_entity_list):\n",
    "        automaton.add_word(entity, (idx, entity))  # 小写加空格形式存入自动机\n",
    "    automaton.make_automaton()\n",
    "    return automaton\n",
    "\n",
    "# 替换标点符号为单个空格\n",
    "def replace_punctuation_with_space(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)  # 替换所有标点符号为单个空格\n",
    "\n",
    "# 清理实体：去掉标点符号\n",
    "def clean_entity(entity):\n",
    "    return re.sub(r'[^\\w\\s]', '', entity).strip()  # 去掉标点符号\n",
    "\n",
    "# 实体识别（直接在原始句子中匹配）\n",
    "def extract_entities_with_aho_corasick(automaton, sentence):\n",
    "    entities = []\n",
    "    sentence_modified = replace_punctuation_with_space(sentence).lower()  # 替换标点符号为单个空格\n",
    "    for end_index, (insert_order, entity) in automaton.iter(sentence_modified):\n",
    "        start_index = end_index - len(entity) + 1\n",
    "        matched_entity = sentence[start_index:end_index + 1]\n",
    "        clean_matched_entity = clean_entity(matched_entity)  # 清理实体，去掉标点\n",
    "        if clean_matched_entity:  # 如果清理后的实体不为空，则添加到结果中\n",
    "            entities.append(clean_matched_entity)\n",
    "    entities = sorted(set(entities), key=len, reverse=True)  # 去重并按长度排序\n",
    "    return entities\n",
    "\n",
    "# 处理单个文件\n",
    "def process_file(file_path, disease_automaton, microbe_automaton):\n",
    "    all_results = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        sentences = nltk.sent_tokenize(text)  # 句子切分\n",
    "        for sentence in sentences:\n",
    "            diseases = extract_entities_with_aho_corasick(disease_automaton, sentence)\n",
    "            microbes = extract_entities_with_aho_corasick(microbe_automaton, sentence)\n",
    "            if diseases and microbes:\n",
    "                for disease in diseases:\n",
    "                    for microbe in microbes:\n",
    "                        all_results.append({\n",
    "                            'DISEASE': disease,\n",
    "                            'MICROBE': microbe,\n",
    "                            'EVIDENCE': sentence.strip(),  # 返回原始句子\n",
    "                            'QUESTIONS': f\"What is the relation between {disease} and {microbe}?\"\n",
    "                        })\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "783d0df7-10b2-4841-afeb-71d363070a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "疾病词典大小: 2287\n",
      "微生物词典大小: 8304\n"
     ]
    }
   ],
   "source": [
    "# 输入和输出目录\n",
    "input_directory = \"/home/GSX/text_mining/pub-abstracts/\"  # 预处理后的文本文件目录\n",
    "output_csv_path = \"/home/GSX/text_mining/pub-abstracts/D-M-SSC.csv\"  # 输出结果CSV文件路径\n",
    "\n",
    "# 微生物字典文件路径\n",
    "microbe_dict_path = '/home/GSX/text_mining/microbe_names.txt'  # 微生物名称词典路径\n",
    "disease_dict_path = '/home/GSX/text_mining/disease_dict.csv'  # 疾病名称词典路径\n",
    "\n",
    "# 加载微生物字典\n",
    "microbe_dict = load_microbe_dict(microbe_dict_path)\n",
    "# 加载疾病字典\n",
    "disease_dict = load_disease_dict_from_csv(disease_dict_path)\n",
    "print(f\"疾病词典大小: {len(disease_dict)}\")\n",
    "print(f\"微生物词典大小: {len(microbe_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e81f7e4-70ad-462e-af63-0d545a3f398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建疾病的Aho-Corasick自动机\n",
    "disease_automaton = build_automaton(disease_dict)\n",
    "\n",
    "# 创建微生物的Aho-Corasick自动机\n",
    "microbe_automaton = build_automaton(microbe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c72878bc-aa0f-41dc-b343-51f12f5c7586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "识别结果已保存到: /home/GSX/text_mining/pub-abstracts/D-M-SSC.csv\n"
     ]
    }
   ],
   "source": [
    "# 用于存储所有文件的识别结果\n",
    "all_results = []\n",
    "\n",
    "target_files = [\"all-microbe-abstracts.txt\"]\n",
    "\n",
    "# 遍历目录下的所有文件\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename in target_files:  \n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # 处理文件\n",
    "        file_results = process_file(file_path, disease_automaton, microbe_automaton)\n",
    "        all_results.extend(file_results)\n",
    "\n",
    "# 创建DataFrame并保存为CSV文件\n",
    "df = pd.DataFrame(all_results)\n",
    "# 去重和清理\n",
    "df['DISEASE'] = df['DISEASE'].str.strip()  # 保留原始形式\n",
    "df['MICROBE'] = df['MICROBE'].str.strip()\n",
    "df['EVIDENCE'] = df['EVIDENCE'].str.strip()\n",
    "df['QUESTIONS'] = df['QUESTIONS'].str.strip()\n",
    "# 大小写无关去重\n",
    "df_lower = df.copy()  # 创建副本用于大小写统一\n",
    "df_lower['DISEASE'] = df_lower['DISEASE'].str.lower()\n",
    "df_lower['MICROBE'] = df_lower['MICROBE'].str.lower()\n",
    "df = df.loc[df_lower.drop_duplicates(subset=['DISEASE', 'MICROBE', 'EVIDENCE']).index]  # 根据去重后的索引保留原始数据\n",
    "\n",
    "# 如果有识别结果，保存到CSV文件\n",
    "if not df.empty:\n",
    "    df.to_csv(output_csv_path, index=False, columns=['DISEASE', 'MICROBE', 'EVIDENCE', 'QUESTIONS'])\n",
    "    print(f\"识别结果已保存到: {output_csv_path}\")\n",
    "else:\n",
    "    print(\"未识别到任何疾病和微生物实体对。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408d03c-9254-4033-b5de-ca39f43a98b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
