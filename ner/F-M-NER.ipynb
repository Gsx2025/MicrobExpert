{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b80666f-60cc-4684-9214-0a0d1ea2d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import ahocorasick\n",
    "import re\n",
    "\n",
    "# 加载食物词典\n",
    "def load_dietary_dict_from_csv(dietary_dict_path):\n",
    "    df = pd.read_csv(dietary_dict_path)\n",
    "    dietary = df['dietary_name'].dropna().unique().tolist()\n",
    "    dietary = [f' {dietary.strip().lower()} ' for dietary in dietary]\n",
    "    return dietary\n",
    "\n",
    "# 从文本文件加载微生物字典\n",
    "def load_microbe_dict(file_path):\n",
    "    microbes = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            microbes.append(f' {line.strip().lower()} ')  # 加入空格防止匹配子串\n",
    "    return microbes\n",
    "\n",
    "# 构建Aho-Corasick自动机\n",
    "def build_automaton(entity_list):\n",
    "    automaton = ahocorasick.Automaton()\n",
    "    sorted_entity_list = sorted(entity_list, key=len, reverse=True)  # 长实体优先\n",
    "    for idx, entity in enumerate(sorted_entity_list):\n",
    "        automaton.add_word(entity, (idx, entity))  # 小写加空格形式存入自动机\n",
    "    automaton.make_automaton()\n",
    "    return automaton\n",
    "\n",
    "# 替换标点符号为单个空格\n",
    "def replace_punctuation_with_space(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)  # 替换所有标点符号为单个空格\n",
    "\n",
    "# 清理实体：去掉标点符号\n",
    "def clean_entity(entity):\n",
    "    return re.sub(r'[^\\w\\s]', '', entity).strip()  # 去掉标点符号\n",
    "\n",
    "# 实体识别（直接在原始句子中匹配）\n",
    "def extract_entities_with_aho_corasick(automaton, sentence):\n",
    "    entities = []\n",
    "    sentence_modified = replace_punctuation_with_space(sentence).lower()  # 替换标点符号为单个空格\n",
    "    for end_index, (insert_order, entity) in automaton.iter(sentence_modified):\n",
    "        start_index = end_index - len(entity) + 1\n",
    "        matched_entity = sentence[start_index:end_index + 1]\n",
    "        clean_matched_entity = clean_entity(matched_entity)  # 清理实体，去掉标点\n",
    "        if clean_matched_entity:  # 如果清理后的实体不为空，则添加到结果中\n",
    "            entities.append(clean_matched_entity)\n",
    "    entities = sorted(set(entities), key=len, reverse=True)  # 去重并按长度排序\n",
    "    return entities\n",
    "\n",
    "# 处理单个文件\n",
    "def process_file(file_path, dietary_automaton, microbe_automaton):\n",
    "    all_results = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        sentences = nltk.sent_tokenize(text)  # 句子切分\n",
    "        for sentence in sentences:\n",
    "            dietary = extract_entities_with_aho_corasick(dietary_automaton, sentence)\n",
    "            microbes = extract_entities_with_aho_corasick(microbe_automaton, sentence)\n",
    "            if dietary and microbes:\n",
    "                for dietary in dietary:\n",
    "                    for microbe in microbes:\n",
    "                        all_results.append({\n",
    "                            'DIETARY': dietary,\n",
    "                            'MICROBE': microbe,\n",
    "                            'EVIDENCE': sentence.strip(),  # 返回原始句子\n",
    "                            'QUESTIONS': f\"What is the relation between {dietary} and {microbe}?\"\n",
    "                        })\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39a8be9f-d0bb-4d2a-b333-c11b09a654e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "食物词典大小: 378\n",
      "微生物词典大小: 8304\n"
     ]
    }
   ],
   "source": [
    "# 输入和输出目录\n",
    "input_directory = \"/home/GSX/text_mining/pub-abstracts/\"  # 预处理后的文本文件目录\n",
    "output_csv_path = \"/home/GSX/text_mining/pub-abstracts/F-M-SSC.csv\"  # 输出结果CSV文件路径\n",
    "\n",
    "# 微生物字典文件路径\n",
    "microbe_dict_path = '/home/GSX/text_mining/microbe_names.txt'  # 微生物名称词典路径\n",
    "dietary_dict_path = '/home/GSX/text_mining/dietary_dict.csv'  # 疾病名称词典路径\n",
    "\n",
    "# 加载微生物字典\n",
    "microbe_dict = load_microbe_dict(microbe_dict_path)\n",
    "# 加载食物字典\n",
    "dietary_dict = load_dietary_dict_from_csv(dietary_dict_path)\n",
    "print(f\"食物词典大小: {len(dietary_dict)}\")\n",
    "print(f\"微生物词典大小: {len(microbe_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a35c728-1df9-4292-89e2-2af1857d6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建食物的Aho-Corasick自动机\n",
    "dietary_automaton = build_automaton(dietary_dict)\n",
    "\n",
    "# 创建微生物的Aho-Corasick自动机\n",
    "microbe_automaton = build_automaton(microbe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ee553e-d45a-4562-9130-973255ffd21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "识别结果已保存到: /home/GSX/text_mining/pub-abstracts/F-M-SSC.csv\n"
     ]
    }
   ],
   "source": [
    "# 用于存储所有文件的识别结果\n",
    "all_results = []\n",
    "\n",
    "target_files = [\"all-microbe-abstracts.txt\"]\n",
    "\n",
    "# 遍历目录下的所有文件\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename in target_files:  \n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # 处理文件\n",
    "        file_results = process_file(file_path, dietary_automaton, microbe_automaton)\n",
    "        all_results.extend(file_results)\n",
    "\n",
    "# 创建DataFrame并保存为CSV文件\n",
    "df = pd.DataFrame(all_results)\n",
    "# 去重和清理\n",
    "df['DIETARY'] = df['DIETARY'].str.strip()  # 保留原始形式\n",
    "df['MICROBE'] = df['MICROBE'].str.strip()\n",
    "df['EVIDENCE'] = df['EVIDENCE'].str.strip()\n",
    "df['QUESTIONS'] = df['QUESTIONS'].str.strip()\n",
    "# 大小写无关去重\n",
    "df_lower = df.copy()  # 创建副本用于大小写统一\n",
    "df_lower['DIETARY'] = df_lower['DIETARY'].str.lower()\n",
    "df_lower['MICROBE'] = df_lower['MICROBE'].str.lower()\n",
    "df = df.loc[df_lower.drop_duplicates(subset=['DIETARY', 'MICROBE', 'EVIDENCE']).index]  # 根据去重后的索引保留原始数据\n",
    "\n",
    "# 如果有识别结果，保存到CSV文件\n",
    "if not df.empty:\n",
    "    df.to_csv(output_csv_path, index=False, columns=['DIETARY', 'MICROBE', 'EVIDENCE', 'QUESTIONS'])\n",
    "    print(f\"识别结果已保存到: {output_csv_path}\")\n",
    "else:\n",
    "    print(\"未识别到任何食物和微生物实体对。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d9973-947c-47b3-a533-43c68e01d3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
